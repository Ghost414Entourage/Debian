sudo apt update  

sudo apt install qemu-kvm libvirt-daemon-system  libvirt-clients bridge-utils virt-manager 

sudo systemctl enable --now libvirtd 

sudo systemctl status libvirtd 

The configuration files for libvirt are stored under  /etc/libvirt, while VM definitions are usually kept in  /etc/libvirt/qemu/. 

sudo usermod -aG libvirt $USER 

Virtual disks are stored as image files, usually in  /var/lib/libvirt/images/

qemu-img resize /var/lib/libvirt/images/ubuntuvm.qcow2 +20G 

sudo wget -O /var/lib/libvirt/images/ubuntu-24.04.3-live-server-amd64.iso \
  https://releases.ubuntu.com/24.04/ubuntu-24.04.3-live-server-amd64.iso


sudo wget -O /var/lib/libvirt/images/debian-12.11.0-amd64-netinst.iso \
  https://cdimage.debian.org/mirror/cdimage/archive/12.11.0/amd64/iso-cd/debian-12.11.0-amd64-netinst.iso






Virtual Disk Formats (qcow2, raw, vmdk)  


Virtual disks are files that act like physical hard  drives for virtual machines. When a VM boots, the  hypervisor presents the virtual disk as though it were  a real drive, but behind the scenes it is simply reading  and writing to a file on the host system. The format  of this file determines how the virtual disk behaves  in terms of performance, features, and compatibility. 

The raw format is the simplest virtual disk format. A  raw disk image is essentially a byte-for-byte  representation of a physical disk. It does not include  compression, snapshots, or advanced features. What  you see is what you get. 

The qcow2 (QEMU Copy-On-Write version 2)  format is the most common format used in KVM  environments. It adds significant functionality on top  of what raw provides. 

The vmdk (Virtual Machine Disk) format was  developed by VMware and is widely used in  VMware products like Workstation, ESXi, and  Fusion. However, it is also supported by QEMU and  KVM. 




Creating Virtual Disks  The qemu-img tool is the most versatile way to create  disk images. It supports multiple formats and allows  you to define size and format options. 

qemu-img create -f qcow2  /var/lib/libvirt/images/vm1.qcow2 20G 

qemu-img create -f raw  /var/lib/libvirt/images/vm2.raw 20G 

qemu-img create -f vmdk  /var/lib/libvirt/images/vm3.vmdk 20G 




qemu-img resize /var/lib/libvirt/images/vm1.qcow2  +10G 

After resizing the image, you must also extend the  filesystem inside the VM to use the additional space.  For Linux guests, this often involves resizing the  partition with fdisk or parted and then extending the  filesystem with tools like resize2fs or xfs_growfs. 

Example for ext4 filesystem:  sudo resize2fs /dev/sda1  

Example for XFS filesystem:  sudo xfs_growfs/ 








Types of storage pools:  

● Dir pool: A simple directory where disk  images are stored.  
● LVM pool: Logical volumes created in an  LVM volume group.  
● iSCSI pool: Network-based storage using  iSCSI protocol.  
● NFS pool: Remote storage shared via NFS.  
● ZFS pool: Based on ZFS datasets. 

Creating a directory-based pool:  

virsh pool-define-as mypool dir --target  
/var/lib/libvirt/images/mypool  
virsh pool-build mypool  
virsh pool-start mypool  
virsh pool-autostart mypool 


Within a pool, volumes are individual storage units  that correspond to virtual disks. For example, in a  directory pool, each qcow2 or raw file is a volume. 

virsh vol-create-as mypool vm-disk.qcow2 20G --  format qcow2 

This creates a 20 GB qcow2 disk in the mypool  storage pool. 

virsh vol-list mypool 

virsh vol-delete vm-disk.qcow2 --pool mypool 



////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Nested virtualization refers to running a hypervisor  inside a virtual machine. In other words, you can  create virtual machines within a VM, which itself  runs on a physical hypervisor. 


echo "options kvm-intel nested=1" | sudo tee  /etc/modprobe.d/kvm-intel.conf 

modprobe -r kvm_intel
modprobe kvm_intel


cat /sys/module/kvm_intel/parameters/nested 

sudo modprobe kvm  
sudo modprobe kvm_intel #for Intel processors  
sudo modprobe kvm_amd #for AMD processors 

Intel VT-x  Intel Virtualization Technology (VT-x) extends the  x86 instruction set to support virtualization. It  enables the hypervisor to run virtual machines with  minimal performance penalties. Without VT-x,  virtualization software may fall back on slower  emulation methods. 

AMD-V  AMD Virtualization (AMD-V) provides similar  functionality on AMD processors. Like VT-x, it adds  instructions that allow efficient execution of  virtualized workloads. 


egrep -c '(vmx|svm)' /proc/cpuinfo 

If the output is greater than zero, your CPU  supports hardware virtualization.  

○ vmx corresponds to Intel VT-x.  
○ svm corresponds to AMD-V. 

lscpu | grep Virtualization 


Using the BIOS/UEFI: Enter your system’s  BIOS or UEFI firmware settings during boot  (often by pressing Del, F2, or F10). Look for  settings labeled Intel VT-x, Intel  Virtualization Technology, SVM (for AMD),  or similar. Ensure they are enabled. 




lsmod | grep kvm  

If they are not loaded automatically at boot, you can  add them to the /etc/modules file (on  Debian/Ubuntu) or configure the appropriate system  service on Red Hat-based distributions. 

sudo usermod -aG kvm $USER  
sudo usermod -aG libvirt $USER 


sudo systemctl start libvirtd  

sudo systemctl enable libvirtd 


Type 1 hypervisors run directly on the host’s  physical hardware without requiring a host operating system. Because they have direct access to the  hardware, they generally offer higher performance,  better scalability, and more advanced security  features than Type 2 hypervisors.  Examples of Type 1 hypervisors include VMware  ESXi, Microsoft Hyper-V in its bare-metal form, and  Xen. In the Linux ecosystem, KVM is sometimes  classified as a Type 1 hypervisor because it integrates  directly into the Linux kernel, though it requires a  Linux host to function. 

Type 2 hypervisors run on top of a conventional host  operating system. The hypervisor functions as an  application that leverages the host OS to access  hardware resources. Examples include Oracle  VirtualBox, VMware Workstation, and Parallels  Desktop.  Because they depend on a host OS, Type 2  hypervisors introduce an extra layer between the  hardware and the virtual machines. This usually  results in slightly lower performance compared to  Type 1 hypervisors, but they are much easier to set  up and use. 

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


sudo qemu-img create -f qcow2 /var/lib/libvirt/images/ubuntu.qcow2 40G

sudo qemu-img create -f qcow2 /var/lib/libvirt/images/debian.qcow2 40G

This is the proper layout for KVM/libvirt.
Here’s what each file actually does:
*.iso → installer media (like a virtual DVD)
*.qcow2 → the VM’s virtual hard drive


sudo rm /var/lib/libvirt/images/debian.qcow2

ls -lh /var/lib/libvirt/images/

sudo qemu-img create -f qcow2 /var/lib/libvirt/images/debian.qcow2 60G

qemu-img info /var/lib/libvirt/images/debian.qcow2



sudo apt install libosinfo-bin
osinfo-query os | grep -i debian
osinfo-db-validate



sudo virsh net-list --all

sudo virsh net-start default
sudo virsh net-autostart default

sudo virsh net-list --all

sudo virsh list --all

sudo virsh shutdown proxmox-nested

sudo virsh undefine proxmox-nested

sudo virsh undefine proxmox-nested --nvram

sudo virsh destroy proxmox-nested

sudo virsh list --all






////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



A Linux bridge acts like a virtual switch, connecting  multiple interfaces together. The brctl tool was historically used for this purpose, but modern  systems use the ip command or nmcli for  NetworkManager-controlled systems. 

Example: Create a simple bridge interface  

sudo ip link add name br0 type bridge  
sudo ip link set br0 up 



You can then attach a physical interface (such as  eth0) to the bridge:  

sudo ip link set eth0 master br0 

This connects the physical interface to the bridge,  allowing virtual interfaces to share the same network. 


Connecting VMs to the Bridge  Libvirt provides a straightforward way to attach VMs  to a bridge through its network XML definition. 


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

For more advanced scenarios, administrators often  use Open vSwitch (OVS). OVS is a productiongrade virtual switch that supports VLANs, tunneling,  and integration with cloud and container  orchestration platforms. 

sudo apt install openvswitch-switch 

sudo ovs-vsctl add-br ovs-br0  
sudo ovs-vsctl add-port ovs-br0 eth0 


virsh net-list --all 

virsh net-start default 

virsh net-destroy  default && virsh net-start default. 

https://github.com/community-scripts/ProxmoxVE

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

sudo wget --no-check-certificate -O proxmox.iso \
  https://download.proxmox.com/iso/proxmox-ve_8.4-1.iso

sha256sum proxmox.iso

https://download.proxmox.com/iso/




